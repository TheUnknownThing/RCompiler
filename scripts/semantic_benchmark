#!/usr/bin/env python3

"""
RCompiler Semantic Analysis Benchmark Script
Tests semantic analysis by comparing compiler exit codes with expected results
"""

import argparse
import json
import os
import subprocess
import sys
from pathlib import Path
from typing import Dict, List, Tuple, Optional

# Colors
class Colors:
    RED = '\033[0;31m'
    GREEN = '\033[0;32m'
    YELLOW = '\033[1;33m'
    BLUE = '\033[0;34m'
    NC = '\033[0m'

def print_success(msg: str) -> None:
    print(f"{Colors.GREEN}✓ {msg}{Colors.NC}")

def print_error(msg: str) -> None:
    print(f"{Colors.RED}✗ {msg}{Colors.NC}")

def print_info(msg: str) -> None:
    print(f"{Colors.BLUE}ℹ {msg}{Colors.NC}")

class TestResult:
    def __init__(self, name: str, stage: str, expected: str, actual: int, 
                 output: str = "", passed: bool = False):
        self.name = name
        self.stage = stage
        self.expected = expected
        self.actual = actual
        self.output = output
        self.passed = passed
        self.full_name = f"{stage}/{name}"

class BenchmarkRunner:
    def __init__(self, args):
        self.build_dir = Path("build")
        self.testcases_dir = Path("testcases")
        self.compiler_exec = "rcompiler"
        self.verbose = args.verbose
        self.quiet = args.quiet
        self.stage_filter = args.stage
        self.testpoint = args.testpoint  # New: specific testpoint filter
        
        self.passed_tests: List[TestResult] = []
        self.failed_tests: List[TestResult] = []

    def check_environment(self) -> None:
        """Check if required tools and files exist"""
        compiler_path = self.build_dir / self.compiler_exec
        if not compiler_path.is_file():
            print_error(f"Compiler not found: {compiler_path}")
            print("Run: cmake --build build")
            sys.exit(1)
        
        if not self.testcases_dir.is_dir():
            print_error(f"Testcases directory not found: {self.testcases_dir}")
            sys.exit(1)

    def find_testcases(self) -> List[Tuple[Path, Dict]]:
        """Find all testcase directories matching the stage and testpoint filters"""
        testcases = []
        
        # Look for global.json in stage directories
        for stage_dir in self.testcases_dir.iterdir():
            if not stage_dir.is_dir() or stage_dir.name == '.git':
                continue
            
            stage_name = stage_dir.name
            
            # Filter by stage
            if self.stage_filter and stage_name != self.stage_filter:
                continue
            
            # Load global.json for this stage
            global_json_path = stage_dir / "global.json"
            if not global_json_path.is_file():
                continue
            
            try:
                with open(global_json_path, 'r') as f:
                    test_metadata_list = json.load(f)
            except (json.JSONDecodeError, FileNotFoundError):
                continue
            
            # Process each test in the global.json
            for test_metadata in test_metadata_list:
                testname = test_metadata.get('name')
                if not testname:
                    continue
                
                # Filter by testpoint (if specified)
                if self.testpoint and testname != self.testpoint:
                    continue
                
                # Check if active
                if not test_metadata.get('active', True):
                    continue
                
                # Construct path to .rx file
                source_info = test_metadata.get('source', [])
                if source_info:
                    rx_file_rel = source_info[0] if isinstance(source_info, list) else source_info
                    rx_file = stage_dir / rx_file_rel
                else:
                    # Fallback to default path
                    rx_file = stage_dir / "src" / testname / f"{testname}.rx"
                
                if rx_file.is_file():
                    testcases.append((rx_file, test_metadata))
        
        return sorted(testcases, key=lambda t: f"{t[0].parts[-3]}/{t[1]['name']}")

    def get_expected_exit_code(self, metadata: Dict) -> Optional[str]:
        """Extract expected exit code from testcase metadata"""
        return metadata.get('compileexitcode', 'UNKNOWN')

    def run_compiler(self, rx_file: Path) -> Tuple[int, str]:
        """Run the compiler and return exit code and output"""
        compiler_path = self.build_dir / self.compiler_exec
        try:
            with open(rx_file, 'r') as f:
                result = subprocess.run(
                    [str(compiler_path)],
                    stdin=f,
                    capture_output=True,
                    text=True
                )
                return result.returncode, result.stderr + result.stdout
        except subprocess.SubprocessError as e:
            return 1, str(e)

    def run_test(self, rx_file: Path, metadata: Dict) -> TestResult:
        """Run a single test and return the result"""
        testname = metadata.get('name', 'unknown')
        # Extract stage from the path (e.g., testcases/semantic-1/src/...)
        stage = rx_file.parts[-4] if len(rx_file.parts) >= 4 else 'unknown'
        
        # Get expected exit code
        expected = self.get_expected_exit_code(metadata)
        if expected is None or expected == "UNKNOWN":
            return TestResult(testname, stage, "UNKNOWN", -1, 
                            "Could not read expected exit code", False)
        
        # Run compiler
        actual, output = self.run_compiler(rx_file)
        
        # Determine if test passed
        passed = False
        if expected == 0 and actual == 0:
            passed = True
        elif expected == -1 and actual != 0:
            passed = True
        
        return TestResult(testname, stage, expected, actual, output, passed)

    def run_all_tests(self) -> None:
        """Run all tests and collect results"""
        testcases = self.find_testcases()
        
        if not testcases:
            if self.testpoint:
                print_error(f"No testcase '{self.testpoint}' found in stage: {self.stage_filter}")
            else:
                print_error(f"No testcases found for stage: {self.stage_filter}")
            sys.exit(1)
        
        if self.testpoint:
            print_info(f"Running testcase '{self.testpoint}' for stage: {self.stage_filter}")
        else:
            print_info(f"Running {len(testcases)} testcases for stage: {self.stage_filter}")
        
        for rx_file, metadata in testcases:
            result = self.run_test(rx_file, metadata)
            
            if result.passed:
                self.passed_tests.append(result)
                if not self.quiet:
                    print_success(result.full_name)
            else:
                self.failed_tests.append(result)
                if not self.quiet:
                    error_msg = f"{result.full_name} (expected: {result.expected}, got: {result.actual})"
                    print_error(error_msg)
                    
            # For single testpoint, always show output if verbose or if test failed
            if self.testpoint and (self.verbose or not result.passed) and result.output:
                print(f"\nCompiler output for {result.full_name}:")
                print("-" * 40)
                print(result.output)
                print("-" * 40)
            elif not self.testpoint and self.verbose and result.output and not result.passed:
                # Show first 5 lines of output for failed tests in multi-test mode
                output_lines = result.output.strip().split('\n')[:5]
                for line in output_lines:
                    print(f"  {line}")

    def print_summary(self) -> None:
        """Print organized test results summary"""
        total_tests = len(self.passed_tests) + len(self.failed_tests)
        pass_rate = (len(self.passed_tests) * 100 // total_tests) if total_tests > 0 else 0
        
        print()
        print("=" * 60)
        print(f"RESULTS: {len(self.passed_tests)}/{total_tests} passed ({pass_rate}%)")
        print("=" * 60)
        
        # Show passed tests (sorted)
        if self.passed_tests:
            print(f"\n{Colors.GREEN}PASSED TESTS ({len(self.passed_tests)}):{Colors.NC}")
            for result in sorted(self.passed_tests, key=lambda r: r.full_name):
                print(f"  ✓ {result.full_name}")
        
        # Show failed tests (sorted)
        if self.failed_tests:
            print(f"\n{Colors.RED}FAILED TESTS ({len(self.failed_tests)}):{Colors.NC}")
            for result in sorted(self.failed_tests, key=lambda r: r.full_name):
                print(f"  ✗ {self.testcases_dir}/{result.full_name}.rx (expected: {result.expected}, got: {result.actual})")
                if self.verbose and result.output:
                    output_preview = result.output.strip().split('\n')[0]
                    print(f"    → {output_preview}")
        
        print()
        
        if not self.failed_tests:
            print_success("All tests passed!")
            sys.exit(0)
        else:
            print_error(f"{len(self.failed_tests)} tests failed")
            sys.exit(1)

def main():
    parser = argparse.ArgumentParser(
        description="Test semantic analysis against testcases",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  %(prog)s                    # Test semantic-1 with summary
  %(prog)s -v                 # Test with verbose output
  %(prog)s -s semantic-2      # Test different stage
  %(prog)s -q                 # Quiet mode
  %(prog)s array2             # Test specific testpoint 'array2' in semantic-1 with verbose output
  %(prog)s array2 -s semantic-2  # Test 'array2' in semantic-2
        """
    )
    
    parser.add_argument('testpoint', nargs='?', 
                        help='Specific testpoint to run (enables verbose output automatically)')
    parser.add_argument('-v', '--verbose', action='store_true',
                        help='Show compiler output')
    parser.add_argument('-s', '--stage', default='semantic-1',
                        help='Test stage (default: semantic-1)')
    parser.add_argument('-q', '--quiet', action='store_true',
                        help='Only show summary')
    
    args = parser.parse_args()
    
    # If testpoint is specified, automatically enable verbose output
    if args.testpoint:
        args.verbose = True
    
    runner = BenchmarkRunner(args)
    runner.check_environment()
    runner.run_all_tests()
    runner.print_summary()

if __name__ == "__main__":
    main()
